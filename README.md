# ğŸ† Workflow de Scraping de panamericana con GitHub Actions y Docker

Este proyecto implementa un flujo automatizado de extracciÃ³n de datos sobre los productos de la pagina web de panamericana, utilizando GitHub Actions para CI/CD, contenerizaciÃ³n con Docker y gestiÃ³n segura de secretos.

---

## âš™ï¸ Estructura del Flujo de Trabajo

El proceso se gestiona mediante un Ãºnico workflow en GitHub Actions: `docker.yml`, que realiza los siguientes pasos:

### 1. PreparaciÃ³n del Entorno
- Ejecuta el flujo en un entorno Linux (`ubuntu-latest`).
- Configura Python 3.9 y sus dependencias con `setup.py`.

### 2. Login y ContenerizaciÃ³n con Docker
- Inicia sesiÃ³n en Docker Hub usando los secretos `DOCKER_USERNAME` y `DOCKER_TOKEN`.
- Construye una imagen Docker (`scraper_docker`) que incluye el script de scraping.
- Ejecuta la imagen para generar automÃ¡ticamente el archivo CSV y la base de datos con los datos extraÃ­dos en la pagina.

### 3. Commit AutomÃ¡tico
- Si hay cambios generados por el scraping, estos se versionan automÃ¡ticamente en el repositorio mediante `git-auto-commit-action`.

---

## ğŸ”’ Requisitos para la ConfiguraciÃ³n

Debes configurar los siguientes **secretos** en GitHub para habilitar el workflow:

- `DOCKER_USERNAME`: Usuario de Docker Hub.
- `DOCKER_TOKEN`: Token de acceso a Docker Hub.

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
scraper-panamericana/
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ docker.yml
â”œâ”€â”€ build/
â”‚ â””â”€â”€ lib/
â”‚ â””â”€â”€ edu_pad/
â”œâ”€â”€ dist/
â”‚ â””â”€â”€ edu_pad-0.0.1-py3.12.egg
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ edu_pad/
â”‚ â”‚ â”œâ”€â”€ scraper/
â”‚ â”‚ â”‚ â”œâ”€â”€ storage/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ db.py
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ excel.py
â”‚ â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”‚ â”œâ”€â”€ scraping.py
â”‚ â”‚ â”‚ â””â”€â”€ utils.py
â”‚ â”‚ â”œâ”€â”€ static/
â”‚ â”‚ â”‚ â”œâ”€â”€ db/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ Productos_Panamericana.db
â”‚ â”‚ â”‚ â””â”€â”€ xlsx/
â”‚ â”‚ â”‚ â””â”€â”€ Productos_Panamericana.xlsx
â”‚ â”‚ â””â”€â”€ main.py # Punto de entrada
â”‚ â””â”€â”€ edu_pad.egg-info/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ accionables.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ setup.py
```

---

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

1. Clona este repositorio:  
   `git clone https://github.com/Saimon7741/scraper-panamericana.git`

2. Configura los secretos necesarios en GitHub.

3. El flujo `docker.yml` se ejecutarÃ¡ automÃ¡ticamente al hacer push al branch `main` o de forma manual desde la interfaz de GitHub Actions.

---

## ğŸŒŸ CaracterÃ­sticas Principales

- **Automatizado**: El scraping, versionado y despliegue se ejecutan sin intervenciÃ³n manual.
- **Contenerizado**: El proceso corre dentro de una imagen Docker reproducible.
- **Seguro**: GestiÃ³n de credenciales mediante secretos de GitHub.
- **Trazable**: Cada ejecuciÃ³n queda registrada y versionada automÃ¡ticamente.

---

## ğŸ› ï¸ PersonalizaciÃ³n

- Puedes modificar la busqueda desde python `Run python src/edu_pad/main.py --search "su busqueda"` o en docker seria: `docker run --rm scraper_docker --search "su busqueda"`, de momento solo se puede buscar solamente la primera palabra del producto.
- Ajusta el `Dockerfile` si necesitas nuevas dependencias o rutas de ejecuciÃ³n.
- Cambia la frecuencia o condiciones del flujo editando `docker.yml`.
